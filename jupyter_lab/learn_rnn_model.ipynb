{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33de5e1f-fffc-4554-bb65-774e937a2922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x00000203AB49C590>\n",
      "Epoch [1/100], Loss: 2.032498776912689\n",
      "Epoch [2/100], Loss: 0.9291759580373764\n",
      "Epoch [3/100], Loss: 0.12778213678393513\n",
      "Epoch [4/100], Loss: 0.22079689428210258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_77448\\3726986710.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_77448\\3726986710.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.11263717338442802\n",
      "Epoch [6/100], Loss: 0.028834990691393614\n",
      "Epoch [7/100], Loss: 0.06898840796202421\n",
      "Epoch [8/100], Loss: 0.04088587174192071\n",
      "Epoch [9/100], Loss: 0.01132285944186151\n",
      "Epoch [10/100], Loss: 0.026359519455581903\n",
      "Epoch [11/100], Loss: 0.019150303909555078\n",
      "Epoch [12/100], Loss: 0.012974777957424521\n",
      "Epoch [13/100], Loss: 0.024822188075631857\n",
      "Epoch [14/100], Loss: 0.011853378149680793\n",
      "Epoch [15/100], Loss: 0.009989956975914538\n",
      "Epoch [16/100], Loss: 0.010450383299030364\n",
      "Epoch [17/100], Loss: 0.010011848760768771\n",
      "Epoch [18/100], Loss: 0.01082534552551806\n",
      "Epoch [19/100], Loss: 0.01150921720545739\n",
      "Epoch [20/100], Loss: 0.009916511364281178\n",
      "Epoch [21/100], Loss: 0.009912413661368191\n",
      "Epoch [22/100], Loss: 0.009175224753562361\n",
      "Epoch [23/100], Loss: 0.008520250616129488\n",
      "Epoch [24/100], Loss: 0.010456001269631088\n",
      "Epoch [25/100], Loss: 0.008758321404457092\n",
      "Epoch [26/100], Loss: 0.008108531008474529\n",
      "Epoch [27/100], Loss: 0.008152462192811072\n",
      "Epoch [28/100], Loss: 0.008653212455101311\n",
      "Epoch [29/100], Loss: 0.009517446625977755\n",
      "Epoch [30/100], Loss: 0.008754522074013948\n",
      "Epoch [31/100], Loss: 0.008301950176246464\n",
      "Epoch [32/100], Loss: 0.01156321435701102\n",
      "Epoch [33/100], Loss: 0.010362809523940086\n",
      "Epoch [34/100], Loss: 0.0076689934940077364\n",
      "Epoch [35/100], Loss: 0.009050125954672694\n",
      "Epoch [36/100], Loss: 0.00968053296674043\n",
      "Epoch [37/100], Loss: 0.008910162141546607\n",
      "Epoch [38/100], Loss: 0.008055456855800003\n",
      "Epoch [39/100], Loss: 0.00782420125324279\n",
      "Epoch [40/100], Loss: 0.008707568747922778\n",
      "Epoch [41/100], Loss: 0.0077971991850063205\n",
      "Epoch [42/100], Loss: 0.008767219725996256\n",
      "Epoch [43/100], Loss: 0.007997544831596315\n",
      "Epoch [44/100], Loss: 0.00963652937207371\n",
      "Epoch [45/100], Loss: 0.01774849696084857\n",
      "Epoch [46/100], Loss: 0.0077651459141634405\n",
      "Epoch [47/100], Loss: 0.007855979551095515\n",
      "Epoch [48/100], Loss: 0.008654118981212378\n",
      "Epoch [49/100], Loss: 0.008695794735103846\n",
      "Epoch [50/100], Loss: 0.008719268720597029\n",
      "Epoch [51/100], Loss: 0.008404228196013719\n",
      "Epoch [52/100], Loss: 0.009157126070931554\n",
      "Epoch [53/100], Loss: 0.00966874579899013\n",
      "Epoch [54/100], Loss: 0.007316159055335447\n",
      "Epoch [55/100], Loss: 0.010419935919344425\n",
      "Epoch [56/100], Loss: 0.009707213845103979\n",
      "Epoch [57/100], Loss: 0.01039662177208811\n",
      "Epoch [58/100], Loss: 0.0077968473779037595\n",
      "Epoch [59/100], Loss: 0.009552279021590948\n",
      "Epoch [60/100], Loss: 0.008421536651439965\n",
      "Epoch [61/100], Loss: 0.00885210174601525\n",
      "Epoch [62/100], Loss: 0.011226119939237833\n",
      "Epoch [63/100], Loss: 0.007883736921939999\n",
      "Epoch [64/100], Loss: 0.009122738963924348\n",
      "Epoch [65/100], Loss: 0.007812924217432737\n",
      "Epoch [66/100], Loss: 0.009168630582280457\n",
      "Epoch [67/100], Loss: 0.010583690134808421\n",
      "Epoch [68/100], Loss: 0.008127639419399202\n",
      "Epoch [69/100], Loss: 0.011401331634260714\n",
      "Epoch [70/100], Loss: 0.00832896213978529\n",
      "Epoch [71/100], Loss: 0.007280601421371102\n",
      "Epoch [72/100], Loss: 0.00757066416554153\n",
      "Epoch [73/100], Loss: 0.0076196613954380155\n",
      "Epoch [74/100], Loss: 0.007163934991694987\n",
      "Epoch [75/100], Loss: 0.008065377245657146\n",
      "Epoch [76/100], Loss: 0.007318185467738658\n",
      "Epoch [77/100], Loss: 0.008449343149550259\n",
      "Epoch [78/100], Loss: 0.00986213970463723\n",
      "Epoch [79/100], Loss: 0.008268802426755428\n",
      "Epoch [80/100], Loss: 0.0074892627308145165\n",
      "Epoch [81/100], Loss: 0.007534789037890732\n",
      "Epoch [82/100], Loss: 0.00858127255924046\n",
      "Epoch [83/100], Loss: 0.008218250004574656\n",
      "Epoch [84/100], Loss: 0.007898424344602972\n",
      "Epoch [85/100], Loss: 0.007140492263715714\n",
      "Epoch [86/100], Loss: 0.00736504839733243\n",
      "Epoch [87/100], Loss: 0.006686104461550713\n",
      "Epoch [88/100], Loss: 0.0072054831543937325\n",
      "Epoch [89/100], Loss: 0.008753167348913848\n",
      "Epoch [90/100], Loss: 0.007216839410830289\n",
      "Epoch [91/100], Loss: 0.008089884300716221\n",
      "Epoch [92/100], Loss: 0.009436488267965615\n",
      "Epoch [93/100], Loss: 0.007574128918349743\n",
      "Epoch [94/100], Loss: 0.00871081615332514\n",
      "Epoch [95/100], Loss: 0.00835737760644406\n",
      "Epoch [96/100], Loss: 0.008679324528202415\n",
      "Epoch [97/100], Loss: 0.0071827194187790155\n",
      "Epoch [98/100], Loss: 0.007209200877696276\n",
      "Epoch [99/100], Loss: 0.00806850683875382\n",
      "Epoch [100/100], Loss: 0.01602518488653004\n",
      "모델 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from rnn_model import RNNModel\n",
    "\n",
    "# 수정된 PaddingCollate 클래스\n",
    "class PaddingCollate:\n",
    "    @staticmethod\n",
    "    def __call__(batch):\n",
    "        time = [item['data'][0] for item in batch]\n",
    "        pin1 = [item['data'][1] for item in batch]\n",
    "        pin2 = [item['data'][2] for item in batch]\n",
    "\n",
    "        max_length_pin1 = max(len(seq) for seq in pin1)\n",
    "        max_length_pin2 = max(len(seq) for seq in pin2)\n",
    "        max_length = max(max_length_pin1, max_length_pin2)\n",
    "\n",
    "        padded_pin1 = []\n",
    "        padded_pin2 = []\n",
    "\n",
    "        for seq1, seq2 in zip(pin1, pin2):\n",
    "            # 리스트를 텐서로 변환하여 패딩 수행\n",
    "            padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
    "            padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n",
    "\n",
    "        time_tensor = torch.tensor(time, dtype=torch.float)\n",
    "        padded_pin1_tensor = torch.stack(padded_pin1)\n",
    "        padded_pin2_tensor = torch.stack(padded_pin2)\n",
    "\n",
    "        return time_tensor, padded_pin1_tensor, padded_pin2_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padding_length = max_length - sequence.size(0)\n",
    "        padded_sequence = torch.nn.functional.pad(sequence, (0, padding_length))\n",
    "        return padded_sequence\n",
    "\n",
    "\n",
    "# 저장된 데이터셋 파일 이름\n",
    "file_name = 'concatenated_dataset'+'.pth'\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = torch.load(file_name)\n",
    "print(dataset)\n",
    "\n",
    "# DataLoader에 로드된 데이터셋 사용\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=PaddingCollate())\n",
    "\n",
    "# 학습 반복 횟수 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델 생성\n",
    "input_size = 2  # 입력 차원 설정 (pin1과 pin2)\n",
    "hidden_size = 64  # 은닉 상태의 크기 설정\n",
    "output_size = 1  # 출력 차원 설정\n",
    "num_layers = 2\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (time, pin1, pin2) in enumerate(dataloader):\n",
    "        # 모델 입력 준비\n",
    "        inputs = torch.stack([pin1, pin2], dim=2)  # pin1과 pin2를 합쳐서 입력으로 사용\n",
    "        inputs = inputs.float()  # 데이터 타입을 float으로 변환\n",
    "        targets = time.float()  # 타겟은 측정 시간으로 설정\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss 계산 및 Backpropagation\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Epoch마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')\n",
    "print(\"모델 저장이 완료되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
