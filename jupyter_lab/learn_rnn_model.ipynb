{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33de5e1f-fffc-4554-bb65-774e937a2922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x000001D8DB99EE50>\n",
      "Epoch [1/100], Loss: 2.5366011261940002\n",
      "Epoch [2/100], Loss: 1.4121361076831818\n",
      "Epoch [3/100], Loss: 0.37228966131806374\n",
      "Epoch [4/100], Loss: 0.1364179728552699\n",
      "Epoch [5/100], Loss: 0.2221512384712696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_122620\\3726986710.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_122620\\3726986710.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.03936552442610264\n",
      "Epoch [7/100], Loss: 0.058854042552411556\n",
      "Epoch [8/100], Loss: 0.06978949997574091\n",
      "Epoch [9/100], Loss: 0.028242580126971006\n",
      "Epoch [10/100], Loss: 0.022198363672941923\n",
      "Epoch [11/100], Loss: 0.03181131603196263\n",
      "Epoch [12/100], Loss: 0.023522928124293685\n",
      "Epoch [13/100], Loss: 0.01812517666257918\n",
      "Epoch [14/100], Loss: 0.02087135473266244\n",
      "Epoch [15/100], Loss: 0.019548729993402958\n",
      "Epoch [16/100], Loss: 0.01758422702550888\n",
      "Epoch [17/100], Loss: 0.01775480480864644\n",
      "Epoch [18/100], Loss: 0.017672873567789793\n",
      "Epoch [19/100], Loss: 0.016710652504116297\n",
      "Epoch [20/100], Loss: 0.0167253315448761\n",
      "Epoch [21/100], Loss: 0.016552595421671867\n",
      "Epoch [22/100], Loss: 0.016313451109454036\n",
      "Epoch [23/100], Loss: 0.01648421515710652\n",
      "Epoch [24/100], Loss: 0.016397927654907107\n",
      "Epoch [25/100], Loss: 0.016350508201867342\n",
      "Epoch [26/100], Loss: 0.01616275729611516\n",
      "Epoch [27/100], Loss: 0.01604279689490795\n",
      "Epoch [28/100], Loss: 0.015958501491695642\n",
      "Epoch [29/100], Loss: 0.015978120965883136\n",
      "Epoch [30/100], Loss: 0.016020899638533592\n",
      "Epoch [31/100], Loss: 0.015816098544746637\n",
      "Epoch [32/100], Loss: 0.01588622690178454\n",
      "Epoch [33/100], Loss: 0.01568140648305416\n",
      "Epoch [34/100], Loss: 0.015699647599831223\n",
      "Epoch [35/100], Loss: 0.01551528088748455\n",
      "Epoch [36/100], Loss: 0.01559718674980104\n",
      "Epoch [37/100], Loss: 0.015396112576127052\n",
      "Epoch [38/100], Loss: 0.015302316984161735\n",
      "Epoch [39/100], Loss: 0.015483969589695334\n",
      "Epoch [40/100], Loss: 0.015284881228581071\n",
      "Epoch [41/100], Loss: 0.015103409998118877\n",
      "Epoch [42/100], Loss: 0.014892351580783725\n",
      "Epoch [43/100], Loss: 0.014709652867168188\n",
      "Epoch [44/100], Loss: 0.014733216492459178\n",
      "Epoch [45/100], Loss: 0.014683162327855825\n",
      "Epoch [46/100], Loss: 0.014622983988374472\n",
      "Epoch [47/100], Loss: 0.014477241784334183\n",
      "Epoch [48/100], Loss: 0.014432211872190237\n",
      "Epoch [49/100], Loss: 0.014441941399127245\n",
      "Epoch [50/100], Loss: 0.01416716817766428\n",
      "Epoch [51/100], Loss: 0.014107228256762028\n",
      "Epoch [52/100], Loss: 0.014079256681725383\n",
      "Epoch [53/100], Loss: 0.01407460798509419\n",
      "Epoch [54/100], Loss: 0.013843768509104848\n",
      "Epoch [55/100], Loss: 0.013897459954023361\n",
      "Epoch [56/100], Loss: 0.01390586863271892\n",
      "Epoch [57/100], Loss: 0.013891947688534856\n",
      "Epoch [58/100], Loss: 0.013576107798144221\n",
      "Epoch [59/100], Loss: 0.01360380370169878\n",
      "Epoch [60/100], Loss: 0.013607822125777602\n",
      "Epoch [61/100], Loss: 0.013447886798530817\n",
      "Epoch [62/100], Loss: 0.013296379707753658\n",
      "Epoch [63/100], Loss: 0.013358993921428919\n",
      "Epoch [64/100], Loss: 0.013420195318758488\n",
      "Epoch [65/100], Loss: 0.013107424601912498\n",
      "Epoch [66/100], Loss: 0.013051238376647234\n",
      "Epoch [67/100], Loss: 0.013015954289585352\n",
      "Epoch [68/100], Loss: 0.012944538611918688\n",
      "Epoch [69/100], Loss: 0.012950935401022434\n",
      "Epoch [70/100], Loss: 0.012767624575644732\n",
      "Epoch [71/100], Loss: 0.0127869404386729\n",
      "Epoch [72/100], Loss: 0.01275841356255114\n",
      "Epoch [73/100], Loss: 0.012586720520630479\n",
      "Epoch [74/100], Loss: 0.012706950772553682\n",
      "Epoch [75/100], Loss: 0.012539553456008434\n",
      "Epoch [76/100], Loss: 0.012530784355476499\n",
      "Epoch [77/100], Loss: 0.012428017565980554\n",
      "Epoch [78/100], Loss: 0.012286335695534945\n",
      "Epoch [79/100], Loss: 0.012164135230705142\n",
      "Epoch [80/100], Loss: 0.012260605581104755\n",
      "Epoch [81/100], Loss: 0.012057393323630095\n",
      "Epoch [82/100], Loss: 0.012008956866338849\n",
      "Epoch [83/100], Loss: 0.011797929182648659\n",
      "Epoch [84/100], Loss: 0.011731327278539538\n",
      "Epoch [85/100], Loss: 0.011681030271574855\n",
      "Epoch [86/100], Loss: 0.01155178202316165\n",
      "Epoch [87/100], Loss: 0.011508838506415486\n",
      "Epoch [88/100], Loss: 0.011306426022201777\n",
      "Epoch [89/100], Loss: 0.011166278971359134\n",
      "Epoch [90/100], Loss: 0.011188425356522202\n",
      "Epoch [91/100], Loss: 0.01100310089532286\n",
      "Epoch [92/100], Loss: 0.010847938945516944\n",
      "Epoch [93/100], Loss: 0.010787050821818411\n",
      "Epoch [94/100], Loss: 0.010746113490313292\n",
      "Epoch [95/100], Loss: 0.010607796255499125\n",
      "Epoch [96/100], Loss: 0.010707243112847209\n",
      "Epoch [97/100], Loss: 0.010521712712943554\n",
      "Epoch [98/100], Loss: 0.010757103329524398\n",
      "Epoch [99/100], Loss: 0.010337040526792407\n",
      "Epoch [100/100], Loss: 0.010301729897037148\n",
      "모델 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from rnn_model import RNNModel\n",
    "\n",
    "# 수정된 PaddingCollate 클래스\n",
    "class PaddingCollate:\n",
    "    @staticmethod\n",
    "    def __call__(batch):\n",
    "        time = [item['data'][0] for item in batch]\n",
    "        pin1 = [item['data'][1] for item in batch]\n",
    "        pin2 = [item['data'][2] for item in batch]\n",
    "\n",
    "        max_length_pin1 = max(len(seq) for seq in pin1)\n",
    "        max_length_pin2 = max(len(seq) for seq in pin2)\n",
    "        max_length = max(max_length_pin1, max_length_pin2)\n",
    "\n",
    "        padded_pin1 = []\n",
    "        padded_pin2 = []\n",
    "\n",
    "        for seq1, seq2 in zip(pin1, pin2):\n",
    "            # 리스트를 텐서로 변환하여 패딩 수행\n",
    "            padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
    "            padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n",
    "\n",
    "        time_tensor = torch.tensor(time, dtype=torch.float)\n",
    "        padded_pin1_tensor = torch.stack(padded_pin1)\n",
    "        padded_pin2_tensor = torch.stack(padded_pin2)\n",
    "\n",
    "        return time_tensor, padded_pin1_tensor, padded_pin2_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padding_length = max_length - sequence.size(0)\n",
    "        padded_sequence = torch.nn.functional.pad(sequence, (0, padding_length))\n",
    "        return padded_sequence\n",
    "\n",
    "\n",
    "# 저장된 데이터셋 파일 이름\n",
    "file_name = 'concatenated_dataset'+'.pth'\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = torch.load(file_name)\n",
    "print(dataset)\n",
    "\n",
    "# DataLoader에 로드된 데이터셋 사용\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=PaddingCollate())\n",
    "\n",
    "# 학습 반복 횟수 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델 생성\n",
    "input_size = 2  # 입력 차원 설정 (pin1과 pin2)\n",
    "hidden_size = 64  # 은닉 상태의 크기 설정\n",
    "output_size = 1  # 출력 차원 설정\n",
    "num_layers = 2\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (time, pin1, pin2) in enumerate(dataloader):\n",
    "        # 모델 입력 준비\n",
    "        inputs = torch.stack([pin1, pin2], dim=2)  # pin1과 pin2를 합쳐서 입력으로 사용\n",
    "        inputs = inputs.float()  # 데이터 타입을 float으로 변환\n",
    "        targets = time.float()  # 타겟은 측정 시간으로 설정\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss 계산 및 Backpropagation\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Epoch마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')\n",
    "print(\"모델 저장이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea44c1d1-4598-4df6-90ab-45357ca0d772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:24:06.855487Z",
     "start_time": "2024-05-10T08:24:04.499078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x164d22110>\n",
      "Epoch [1/100], Loss: 2.1888524770736693\n",
      "Epoch [2/100], Loss: 0.9601564645767212\n",
      "Epoch [3/100], Loss: 0.12276406399905682\n",
      "Epoch [4/100], Loss: 0.19288038313388825\n",
      "Epoch [5/100], Loss: 0.12196135967969894\n",
      "Epoch [6/100], Loss: 0.027987163886427878\n",
      "Epoch [7/100], Loss: 0.0495876282453537\n",
      "Epoch [8/100], Loss: 0.03968873582780361\n",
      "Epoch [9/100], Loss: 0.02050492763519287\n",
      "Epoch [10/100], Loss: 0.023515113070607184\n",
      "Epoch [11/100], Loss: 0.022517291083931922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/07/zn9pcs3x2k73h6s55801xzfr0000gn/T/ipykernel_82761/1795809899.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
      "/var/folders/07/zn9pcs3x2k73h6s55801xzfr0000gn/T/ipykernel_82761/1795809899.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.018279915675520897\n",
      "Epoch [13/100], Loss: 0.01869603954255581\n",
      "Epoch [14/100], Loss: 0.018366300500929356\n",
      "Epoch [15/100], Loss: 0.01777171213179827\n",
      "Epoch [16/100], Loss: 0.01727298814803362\n",
      "Epoch [17/100], Loss: 0.01689242012798786\n",
      "Epoch [18/100], Loss: 0.016749514639377593\n",
      "Epoch [19/100], Loss: 0.016737064719200133\n",
      "Epoch [20/100], Loss: 0.016555852442979812\n",
      "Epoch [21/100], Loss: 0.016484660282731056\n",
      "Epoch [22/100], Loss: 0.01634981818497181\n",
      "Epoch [23/100], Loss: 0.01633399948477745\n",
      "Epoch [24/100], Loss: 0.016179878450930118\n",
      "Epoch [25/100], Loss: 0.016180213913321494\n",
      "Epoch [26/100], Loss: 0.016078860498964787\n",
      "Epoch [27/100], Loss: 0.01602744422852993\n",
      "Epoch [28/100], Loss: 0.015990063920617102\n",
      "Epoch [29/100], Loss: 0.015984495915472508\n",
      "Epoch [30/100], Loss: 0.01588781625032425\n",
      "Epoch [31/100], Loss: 0.015794379636645317\n",
      "Epoch [32/100], Loss: 0.015803500637412073\n",
      "Epoch [33/100], Loss: 0.01558336429297924\n",
      "Epoch [34/100], Loss: 0.015680077113211156\n",
      "Epoch [35/100], Loss: 0.015516090206801891\n",
      "Epoch [36/100], Loss: 0.015472139604389668\n",
      "Epoch [37/100], Loss: 0.015318694151937961\n",
      "Epoch [38/100], Loss: 0.015188543684780597\n",
      "Epoch [39/100], Loss: 0.015061881765723229\n",
      "Epoch [40/100], Loss: 0.014924286678433419\n",
      "Epoch [41/100], Loss: 0.014748045802116394\n",
      "Epoch [42/100], Loss: 0.014549868740141391\n",
      "Epoch [43/100], Loss: 0.01441404763609171\n",
      "Epoch [44/100], Loss: 0.014114063978195191\n",
      "Epoch [45/100], Loss: 0.013932568579912185\n",
      "Epoch [46/100], Loss: 0.013663431257009506\n",
      "Epoch [47/100], Loss: 0.013658487237989902\n",
      "Epoch [48/100], Loss: 0.013455740548670292\n",
      "Epoch [49/100], Loss: 0.013230991177260875\n",
      "Epoch [50/100], Loss: 0.012993768230080605\n",
      "Epoch [51/100], Loss: 0.012852564640343189\n",
      "Epoch [52/100], Loss: 0.012732587940990924\n",
      "Epoch [53/100], Loss: 0.012661358714103699\n",
      "Epoch [54/100], Loss: 0.012540488690137862\n",
      "Epoch [55/100], Loss: 0.01244213879108429\n",
      "Epoch [56/100], Loss: 0.01253316830843687\n",
      "Epoch [57/100], Loss: 0.012204955890774727\n",
      "Epoch [58/100], Loss: 0.012123921141028404\n",
      "Epoch [59/100], Loss: 0.011885198391973972\n",
      "Epoch [60/100], Loss: 0.01178735215216875\n",
      "Epoch [61/100], Loss: 0.011649258434772491\n",
      "Epoch [62/100], Loss: 0.011530480533838271\n",
      "Epoch [63/100], Loss: 0.01132415384054184\n",
      "Epoch [64/100], Loss: 0.011226070486009121\n",
      "Epoch [65/100], Loss: 0.011110935267060995\n",
      "Epoch [66/100], Loss: 0.01089202780276537\n",
      "Epoch [67/100], Loss: 0.010912156198173761\n",
      "Epoch [68/100], Loss: 0.010661067999899387\n",
      "Epoch [69/100], Loss: 0.010802296549081802\n",
      "Epoch [70/100], Loss: 0.010689344070851804\n",
      "Epoch [71/100], Loss: 0.010735079552978278\n",
      "Epoch [72/100], Loss: 0.010512338392436505\n",
      "Epoch [73/100], Loss: 0.010341690108180045\n",
      "Epoch [74/100], Loss: 0.01008388763293624\n",
      "Epoch [75/100], Loss: 0.010202163085341453\n",
      "Epoch [76/100], Loss: 0.010201627574861049\n",
      "Epoch [77/100], Loss: 0.010160424932837486\n",
      "Epoch [78/100], Loss: 0.010137928836047649\n",
      "Epoch [79/100], Loss: 0.010368561930954456\n",
      "Epoch [80/100], Loss: 0.010181798972189427\n",
      "Epoch [81/100], Loss: 0.00990898311138153\n",
      "Epoch [82/100], Loss: 0.009973269421607256\n",
      "Epoch [83/100], Loss: 0.009943171218037606\n",
      "Epoch [84/100], Loss: 0.009898586850613355\n",
      "Epoch [85/100], Loss: 0.010368978884071112\n",
      "Epoch [86/100], Loss: 0.010025675408542156\n",
      "Epoch [87/100], Loss: 0.009572182968258857\n",
      "Epoch [88/100], Loss: 0.009413059428334236\n",
      "Epoch [89/100], Loss: 0.009616098459810018\n",
      "Epoch [90/100], Loss: 0.009560075961053372\n",
      "Epoch [91/100], Loss: 0.009488210827112199\n",
      "Epoch [92/100], Loss: 0.009881421644240617\n",
      "Epoch [93/100], Loss: 0.009500569850206374\n",
      "Epoch [94/100], Loss: 0.009231848828494548\n",
      "Epoch [95/100], Loss: 0.009183386992663144\n",
      "Epoch [96/100], Loss: 0.008891182392835617\n",
      "Epoch [97/100], Loss: 0.009118691366165877\n",
      "Epoch [98/100], Loss: 0.00911692315712571\n",
      "Epoch [99/100], Loss: 0.009281879384070634\n",
      "Epoch [100/100], Loss: 0.008897613454610109\n",
      "모델 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from rnn_model import RNNModel\n",
    "\n",
    "# 수정된 PaddingCollate 클래스\n",
    "class PaddingCollate:\n",
    "    @staticmethod\n",
    "    def __call__(batch):\n",
    "        time = [item['data'][0] for item in batch]\n",
    "        pin1 = [item['data'][1] for item in batch]\n",
    "        pin2 = [item['data'][2] for item in batch]\n",
    "\n",
    "        max_length_pin1 = max(len(seq) for seq in pin1)\n",
    "        max_length_pin2 = max(len(seq) for seq in pin2)\n",
    "        max_length = max(max_length_pin1, max_length_pin2)\n",
    "\n",
    "        padded_pin1 = []\n",
    "        padded_pin2 = []\n",
    "\n",
    "        for seq1, seq2 in zip(pin1, pin2):\n",
    "            # 리스트를 텐서로 변환하여 패딩 수행\n",
    "            padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
    "            padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n",
    "\n",
    "        time_tensor = torch.tensor(time, dtype=torch.float)\n",
    "        padded_pin1_tensor = torch.stack(padded_pin1)\n",
    "        padded_pin2_tensor = torch.stack(padded_pin2)\n",
    "\n",
    "        return time_tensor, padded_pin1_tensor, padded_pin2_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padding_length = max_length - sequence.size(0)\n",
    "        padded_sequence = torch.nn.functional.pad(sequence, (0, padding_length))\n",
    "        return padded_sequence\n",
    "\n",
    "\n",
    "# 저장된 데이터셋 파일 이름\n",
    "file_name = 'concatenated_dataset'+'.pth'\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = torch.load(file_name)\n",
    "print(dataset)\n",
    "\n",
    "# DataLoader에 로드된 데이터셋 사용\n",
    "batch_size = 50  # 배치 크기 설정\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=PaddingCollate())\n",
    "\n",
    "# 학습 반복 횟수 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델 생성\n",
    "input_size = 2  # 입력 차원 설정 (pin1과 pin2)\n",
    "hidden_size = 100  # 은닉 상태의 크기 설정\n",
    "output_size = 1  # 출력 차원 설정\n",
    "num_layers = 2\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (time, pin1, pin2) in enumerate(dataloader):\n",
    "        # 모델 입력 준비\n",
    "        inputs = torch.stack([pin1, pin2], dim=2)  # pin1과 pin2를 합쳐서 입력으로 사용\n",
    "        inputs = inputs.float()  # 데이터 타입을 float으로 변환\n",
    "        targets = time.float()  # 타겟은 측정 시간으로 설정\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss 계산 및 Backpropagation\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Epoch마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')\n",
    "print(\"모델 저장이 완료되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
