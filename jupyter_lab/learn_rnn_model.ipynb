{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33de5e1f-fffc-4554-bb65-774e937a2922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x000001D8DB99EE50>\n",
      "Epoch [1/100], Loss: 2.5366011261940002\n",
      "Epoch [2/100], Loss: 1.4121361076831818\n",
      "Epoch [3/100], Loss: 0.37228966131806374\n",
      "Epoch [4/100], Loss: 0.1364179728552699\n",
      "Epoch [5/100], Loss: 0.2221512384712696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_122620\\3726986710.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_122620\\3726986710.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.03936552442610264\n",
      "Epoch [7/100], Loss: 0.058854042552411556\n",
      "Epoch [8/100], Loss: 0.06978949997574091\n",
      "Epoch [9/100], Loss: 0.028242580126971006\n",
      "Epoch [10/100], Loss: 0.022198363672941923\n",
      "Epoch [11/100], Loss: 0.03181131603196263\n",
      "Epoch [12/100], Loss: 0.023522928124293685\n",
      "Epoch [13/100], Loss: 0.01812517666257918\n",
      "Epoch [14/100], Loss: 0.02087135473266244\n",
      "Epoch [15/100], Loss: 0.019548729993402958\n",
      "Epoch [16/100], Loss: 0.01758422702550888\n",
      "Epoch [17/100], Loss: 0.01775480480864644\n",
      "Epoch [18/100], Loss: 0.017672873567789793\n",
      "Epoch [19/100], Loss: 0.016710652504116297\n",
      "Epoch [20/100], Loss: 0.0167253315448761\n",
      "Epoch [21/100], Loss: 0.016552595421671867\n",
      "Epoch [22/100], Loss: 0.016313451109454036\n",
      "Epoch [23/100], Loss: 0.01648421515710652\n",
      "Epoch [24/100], Loss: 0.016397927654907107\n",
      "Epoch [25/100], Loss: 0.016350508201867342\n",
      "Epoch [26/100], Loss: 0.01616275729611516\n",
      "Epoch [27/100], Loss: 0.01604279689490795\n",
      "Epoch [28/100], Loss: 0.015958501491695642\n",
      "Epoch [29/100], Loss: 0.015978120965883136\n",
      "Epoch [30/100], Loss: 0.016020899638533592\n",
      "Epoch [31/100], Loss: 0.015816098544746637\n",
      "Epoch [32/100], Loss: 0.01588622690178454\n",
      "Epoch [33/100], Loss: 0.01568140648305416\n",
      "Epoch [34/100], Loss: 0.015699647599831223\n",
      "Epoch [35/100], Loss: 0.01551528088748455\n",
      "Epoch [36/100], Loss: 0.01559718674980104\n",
      "Epoch [37/100], Loss: 0.015396112576127052\n",
      "Epoch [38/100], Loss: 0.015302316984161735\n",
      "Epoch [39/100], Loss: 0.015483969589695334\n",
      "Epoch [40/100], Loss: 0.015284881228581071\n",
      "Epoch [41/100], Loss: 0.015103409998118877\n",
      "Epoch [42/100], Loss: 0.014892351580783725\n",
      "Epoch [43/100], Loss: 0.014709652867168188\n",
      "Epoch [44/100], Loss: 0.014733216492459178\n",
      "Epoch [45/100], Loss: 0.014683162327855825\n",
      "Epoch [46/100], Loss: 0.014622983988374472\n",
      "Epoch [47/100], Loss: 0.014477241784334183\n",
      "Epoch [48/100], Loss: 0.014432211872190237\n",
      "Epoch [49/100], Loss: 0.014441941399127245\n",
      "Epoch [50/100], Loss: 0.01416716817766428\n",
      "Epoch [51/100], Loss: 0.014107228256762028\n",
      "Epoch [52/100], Loss: 0.014079256681725383\n",
      "Epoch [53/100], Loss: 0.01407460798509419\n",
      "Epoch [54/100], Loss: 0.013843768509104848\n",
      "Epoch [55/100], Loss: 0.013897459954023361\n",
      "Epoch [56/100], Loss: 0.01390586863271892\n",
      "Epoch [57/100], Loss: 0.013891947688534856\n",
      "Epoch [58/100], Loss: 0.013576107798144221\n",
      "Epoch [59/100], Loss: 0.01360380370169878\n",
      "Epoch [60/100], Loss: 0.013607822125777602\n",
      "Epoch [61/100], Loss: 0.013447886798530817\n",
      "Epoch [62/100], Loss: 0.013296379707753658\n",
      "Epoch [63/100], Loss: 0.013358993921428919\n",
      "Epoch [64/100], Loss: 0.013420195318758488\n",
      "Epoch [65/100], Loss: 0.013107424601912498\n",
      "Epoch [66/100], Loss: 0.013051238376647234\n",
      "Epoch [67/100], Loss: 0.013015954289585352\n",
      "Epoch [68/100], Loss: 0.012944538611918688\n",
      "Epoch [69/100], Loss: 0.012950935401022434\n",
      "Epoch [70/100], Loss: 0.012767624575644732\n",
      "Epoch [71/100], Loss: 0.0127869404386729\n",
      "Epoch [72/100], Loss: 0.01275841356255114\n",
      "Epoch [73/100], Loss: 0.012586720520630479\n",
      "Epoch [74/100], Loss: 0.012706950772553682\n",
      "Epoch [75/100], Loss: 0.012539553456008434\n",
      "Epoch [76/100], Loss: 0.012530784355476499\n",
      "Epoch [77/100], Loss: 0.012428017565980554\n",
      "Epoch [78/100], Loss: 0.012286335695534945\n",
      "Epoch [79/100], Loss: 0.012164135230705142\n",
      "Epoch [80/100], Loss: 0.012260605581104755\n",
      "Epoch [81/100], Loss: 0.012057393323630095\n",
      "Epoch [82/100], Loss: 0.012008956866338849\n",
      "Epoch [83/100], Loss: 0.011797929182648659\n",
      "Epoch [84/100], Loss: 0.011731327278539538\n",
      "Epoch [85/100], Loss: 0.011681030271574855\n",
      "Epoch [86/100], Loss: 0.01155178202316165\n",
      "Epoch [87/100], Loss: 0.011508838506415486\n",
      "Epoch [88/100], Loss: 0.011306426022201777\n",
      "Epoch [89/100], Loss: 0.011166278971359134\n",
      "Epoch [90/100], Loss: 0.011188425356522202\n",
      "Epoch [91/100], Loss: 0.01100310089532286\n",
      "Epoch [92/100], Loss: 0.010847938945516944\n",
      "Epoch [93/100], Loss: 0.010787050821818411\n",
      "Epoch [94/100], Loss: 0.010746113490313292\n",
      "Epoch [95/100], Loss: 0.010607796255499125\n",
      "Epoch [96/100], Loss: 0.010707243112847209\n",
      "Epoch [97/100], Loss: 0.010521712712943554\n",
      "Epoch [98/100], Loss: 0.010757103329524398\n",
      "Epoch [99/100], Loss: 0.010337040526792407\n",
      "Epoch [100/100], Loss: 0.010301729897037148\n",
      "모델 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from rnn_model import RNNModel\n",
    "\n",
    "# 수정된 PaddingCollate 클래스\n",
    "class PaddingCollate:\n",
    "    @staticmethod\n",
    "    def __call__(batch):\n",
    "        time = [item['data'][0] for item in batch]\n",
    "        pin1 = [item['data'][1] for item in batch]\n",
    "        pin2 = [item['data'][2] for item in batch]\n",
    "\n",
    "        max_length_pin1 = max(len(seq) for seq in pin1)\n",
    "        max_length_pin2 = max(len(seq) for seq in pin2)\n",
    "        max_length = max(max_length_pin1, max_length_pin2)\n",
    "\n",
    "        padded_pin1 = []\n",
    "        padded_pin2 = []\n",
    "\n",
    "        for seq1, seq2 in zip(pin1, pin2):\n",
    "            # 리스트를 텐서로 변환하여 패딩 수행\n",
    "            padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
    "            padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n",
    "\n",
    "        time_tensor = torch.tensor(time, dtype=torch.float)\n",
    "        padded_pin1_tensor = torch.stack(padded_pin1)\n",
    "        padded_pin2_tensor = torch.stack(padded_pin2)\n",
    "\n",
    "        return time_tensor, padded_pin1_tensor, padded_pin2_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padding_length = max_length - sequence.size(0)\n",
    "        padded_sequence = torch.nn.functional.pad(sequence, (0, padding_length))\n",
    "        return padded_sequence\n",
    "\n",
    "\n",
    "# 저장된 데이터셋 파일 이름\n",
    "file_name = 'concatenated_dataset'+'.pth'\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = torch.load(file_name)\n",
    "print(dataset)\n",
    "\n",
    "# DataLoader에 로드된 데이터셋 사용\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=PaddingCollate())\n",
    "\n",
    "# 학습 반복 횟수 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델 생성\n",
    "input_size = 2  # 입력 차원 설정 (pin1과 pin2)\n",
    "hidden_size = 64  # 은닉 상태의 크기 설정\n",
    "output_size = 1  # 출력 차원 설정\n",
    "num_layers = 2\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (time, pin1, pin2) in enumerate(dataloader):\n",
    "        # 모델 입력 준비\n",
    "        inputs = torch.stack([pin1, pin2], dim=2)  # pin1과 pin2를 합쳐서 입력으로 사용\n",
    "        inputs = inputs.float()  # 데이터 타입을 float으로 변환\n",
    "        targets = time.float()  # 타겟은 측정 시간으로 설정\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss 계산 및 Backpropagation\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Epoch마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')\n",
    "print(\"모델 저장이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea44c1d1-4598-4df6-90ab-45357ca0d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x000001D8DB99D5D0>\n",
      "Epoch [1/100], Loss: 1.5436136722564697\n",
      "Epoch [2/100], Loss: 0.480072882771492\n",
      "Epoch [3/100], Loss: 0.06180946938693523\n",
      "Epoch [4/100], Loss: 0.15235181748867035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_122620\\2169376944.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_122620\\2169376944.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.06093480475246906\n",
      "Epoch [6/100], Loss: 0.02264699451625347\n",
      "Epoch [7/100], Loss: 0.040650004893541335\n",
      "Epoch [8/100], Loss: 0.03085224740207195\n",
      "Epoch [9/100], Loss: 0.018884951807558538\n",
      "Epoch [10/100], Loss: 0.021866797283291817\n",
      "Epoch [11/100], Loss: 0.020743385516107084\n",
      "Epoch [12/100], Loss: 0.018189794570207595\n",
      "Epoch [13/100], Loss: 0.01860718633979559\n",
      "Epoch [14/100], Loss: 0.018354412727057935\n",
      "Epoch [15/100], Loss: 0.017568081244826318\n",
      "Epoch [16/100], Loss: 0.01776074431836605\n",
      "Epoch [17/100], Loss: 0.017426693066954612\n",
      "Epoch [18/100], Loss: 0.01716557703912258\n",
      "Epoch [19/100], Loss: 0.017192034982144833\n",
      "Epoch [20/100], Loss: 0.017048465088009834\n",
      "Epoch [21/100], Loss: 0.01688509341329336\n",
      "Epoch [22/100], Loss: 0.016784829273819923\n",
      "Epoch [23/100], Loss: 0.01680669654160738\n",
      "Epoch [24/100], Loss: 0.016618044674396516\n",
      "Epoch [25/100], Loss: 0.016462823562324046\n",
      "Epoch [26/100], Loss: 0.016418404318392275\n",
      "Epoch [27/100], Loss: 0.016306815110146998\n",
      "Epoch [28/100], Loss: 0.016188302636146547\n",
      "Epoch [29/100], Loss: 0.016235475055873395\n",
      "Epoch [30/100], Loss: 0.015936692990362645\n",
      "Epoch [31/100], Loss: 0.015844833850860596\n",
      "Epoch [32/100], Loss: 0.01564302295446396\n",
      "Epoch [33/100], Loss: 0.015462346747517586\n",
      "Epoch [34/100], Loss: 0.015215878561139106\n",
      "Epoch [35/100], Loss: 0.015019015222787858\n",
      "Epoch [36/100], Loss: 0.014775667898356914\n",
      "Epoch [37/100], Loss: 0.01458025984466076\n",
      "Epoch [38/100], Loss: 0.014355677366256713\n",
      "Epoch [39/100], Loss: 0.01426953226327896\n",
      "Epoch [40/100], Loss: 0.014079583808779716\n",
      "Epoch [41/100], Loss: 0.014007982052862644\n",
      "Epoch [42/100], Loss: 0.013736126571893692\n",
      "Epoch [43/100], Loss: 0.013606376387178898\n",
      "Epoch [44/100], Loss: 0.013477663695812225\n",
      "Epoch [45/100], Loss: 0.013327511213719844\n",
      "Epoch [46/100], Loss: 0.013149646669626236\n",
      "Epoch [47/100], Loss: 0.012964677438139916\n",
      "Epoch [48/100], Loss: 0.012998553551733493\n",
      "Epoch [49/100], Loss: 0.012906893715262413\n",
      "Epoch [50/100], Loss: 0.012607807666063309\n",
      "Epoch [51/100], Loss: 0.012383036594837905\n",
      "Epoch [52/100], Loss: 0.012209964543581009\n",
      "Epoch [53/100], Loss: 0.01199352378025651\n",
      "Epoch [54/100], Loss: 0.011806041467934847\n",
      "Epoch [55/100], Loss: 0.011595463100820779\n",
      "Epoch [56/100], Loss: 0.01135396035388112\n",
      "Epoch [57/100], Loss: 0.011294307839125394\n",
      "Epoch [58/100], Loss: 0.011078546103090049\n",
      "Epoch [59/100], Loss: 0.010960003826767206\n",
      "Epoch [60/100], Loss: 0.011102973856031895\n",
      "Epoch [61/100], Loss: 0.010762758646160364\n",
      "Epoch [62/100], Loss: 0.010971293691545725\n",
      "Epoch [63/100], Loss: 0.010654961317777633\n",
      "Epoch [64/100], Loss: 0.010519711673259735\n",
      "Epoch [65/100], Loss: 0.010550969187170267\n",
      "Epoch [66/100], Loss: 0.010460195876657963\n",
      "Epoch [67/100], Loss: 0.010396461188793182\n",
      "Epoch [68/100], Loss: 0.010363015066832303\n",
      "Epoch [69/100], Loss: 0.010130510572344065\n",
      "Epoch [70/100], Loss: 0.010090997908264399\n",
      "Epoch [71/100], Loss: 0.01004443047568202\n",
      "Epoch [72/100], Loss: 0.00999082876369357\n",
      "Epoch [73/100], Loss: 0.010131168272346258\n",
      "Epoch [74/100], Loss: 0.009902807697653771\n",
      "Epoch [75/100], Loss: 0.010045506246387958\n",
      "Epoch [76/100], Loss: 0.009776746202260257\n",
      "Epoch [77/100], Loss: 0.009922494553029538\n",
      "Epoch [78/100], Loss: 0.009808781277388334\n",
      "Epoch [79/100], Loss: 0.009380795247852802\n",
      "Epoch [80/100], Loss: 0.009692087210714817\n",
      "Epoch [81/100], Loss: 0.009375295601785183\n",
      "Epoch [82/100], Loss: 0.009540660306811333\n",
      "Epoch [83/100], Loss: 0.009462659806013107\n",
      "Epoch [84/100], Loss: 0.00912837740033865\n",
      "Epoch [85/100], Loss: 0.009223387949168682\n",
      "Epoch [86/100], Loss: 0.00930200880393386\n",
      "Epoch [87/100], Loss: 0.009264286607503891\n",
      "Epoch [88/100], Loss: 0.00920746773481369\n",
      "Epoch [89/100], Loss: 0.009182151965796947\n",
      "Epoch [90/100], Loss: 0.009486336261034012\n",
      "Epoch [91/100], Loss: 0.009267289750277996\n",
      "Epoch [92/100], Loss: 0.009810976404696702\n",
      "Epoch [93/100], Loss: 0.009490375313907862\n",
      "Epoch [94/100], Loss: 0.009043156541883946\n",
      "Epoch [95/100], Loss: 0.009172520693391561\n",
      "Epoch [96/100], Loss: 0.00878349794074893\n",
      "Epoch [97/100], Loss: 0.009149274788796902\n",
      "Epoch [98/100], Loss: 0.009087996184825897\n",
      "Epoch [99/100], Loss: 0.009139732643961906\n",
      "Epoch [100/100], Loss: 0.008751171082258225\n",
      "모델 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from rnn_model import RNNModel\n",
    "\n",
    "# 수정된 PaddingCollate 클래스\n",
    "class PaddingCollate:\n",
    "    @staticmethod\n",
    "    def __call__(batch):\n",
    "        time = [item['data'][0] for item in batch]\n",
    "        pin1 = [item['data'][1] for item in batch]\n",
    "        pin2 = [item['data'][2] for item in batch]\n",
    "\n",
    "        max_length_pin1 = max(len(seq) for seq in pin1)\n",
    "        max_length_pin2 = max(len(seq) for seq in pin2)\n",
    "        max_length = max(max_length_pin1, max_length_pin2)\n",
    "\n",
    "        padded_pin1 = []\n",
    "        padded_pin2 = []\n",
    "\n",
    "        for seq1, seq2 in zip(pin1, pin2):\n",
    "            # 리스트를 텐서로 변환하여 패딩 수행\n",
    "            padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
    "            padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n",
    "\n",
    "        time_tensor = torch.tensor(time, dtype=torch.float)\n",
    "        padded_pin1_tensor = torch.stack(padded_pin1)\n",
    "        padded_pin2_tensor = torch.stack(padded_pin2)\n",
    "\n",
    "        return time_tensor, padded_pin1_tensor, padded_pin2_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padding_length = max_length - sequence.size(0)\n",
    "        padded_sequence = torch.nn.functional.pad(sequence, (0, padding_length))\n",
    "        return padded_sequence\n",
    "\n",
    "\n",
    "# 저장된 데이터셋 파일 이름\n",
    "file_name = 'concatenated_dataset'+'.pth'\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = torch.load(file_name)\n",
    "print(dataset)\n",
    "\n",
    "# DataLoader에 로드된 데이터셋 사용\n",
    "batch_size = 50  # 배치 크기 설정\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=PaddingCollate())\n",
    "\n",
    "# 학습 반복 횟수 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델 생성\n",
    "input_size = 2  # 입력 차원 설정 (pin1과 pin2)\n",
    "hidden_size = 50  # 은닉 상태의 크기 설정\n",
    "output_size = 1  # 출력 차원 설정\n",
    "num_layers = 2\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (time, pin1, pin2) in enumerate(dataloader):\n",
    "        # 모델 입력 준비\n",
    "        inputs = torch.stack([pin1, pin2], dim=2)  # pin1과 pin2를 합쳐서 입력으로 사용\n",
    "        inputs = inputs.float()  # 데이터 타입을 float으로 변환\n",
    "        targets = time.float()  # 타겟은 측정 시간으로 설정\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss 계산 및 Backpropagation\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Epoch마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')\n",
    "print(\"모델 저장이 완료되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
