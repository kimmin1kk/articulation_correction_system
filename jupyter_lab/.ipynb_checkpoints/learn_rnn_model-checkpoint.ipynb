{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33de5e1f-fffc-4554-bb65-774e937a2922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x000001B70CDC3490>\n",
      "Epoch [1/100], Loss: 1.9559146165847778\n",
      "Epoch [2/100], Loss: 0.7151312753558159\n",
      "Epoch [3/100], Loss: 0.07601226540282369\n",
      "Epoch [4/100], Loss: 0.23137491941452026\n",
      "Epoch [5/100], Loss: 0.13526099734008312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_29556\\3726986710.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_29556\\3726986710.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.030683457385748625\n",
      "Epoch [7/100], Loss: 0.05991321709007025\n",
      "Epoch [8/100], Loss: 0.055858511477708817\n",
      "Epoch [9/100], Loss: 0.02331153373233974\n",
      "Epoch [10/100], Loss: 0.023192128632217646\n",
      "Epoch [11/100], Loss: 0.028988437727093697\n",
      "Epoch [12/100], Loss: 0.021755092777311802\n",
      "Epoch [13/100], Loss: 0.01798155950382352\n",
      "Epoch [14/100], Loss: 0.019690181128680706\n",
      "Epoch [15/100], Loss: 0.018898068461567163\n",
      "Epoch [16/100], Loss: 0.017581256106495857\n",
      "Epoch [17/100], Loss: 0.017645907355472445\n",
      "Epoch [18/100], Loss: 0.017024048138409853\n",
      "Epoch [19/100], Loss: 0.01673160563223064\n",
      "Epoch [20/100], Loss: 0.016853716457262635\n",
      "Epoch [21/100], Loss: 0.01657455856911838\n",
      "Epoch [22/100], Loss: 0.016466653207316995\n",
      "Epoch [23/100], Loss: 0.016342292074114084\n",
      "Epoch [24/100], Loss: 0.01617604843340814\n",
      "Epoch [25/100], Loss: 0.016087801661342382\n",
      "Epoch [26/100], Loss: 0.016134120989590883\n",
      "Epoch [27/100], Loss: 0.01603808137588203\n",
      "Epoch [28/100], Loss: 0.015785977011546493\n",
      "Epoch [29/100], Loss: 0.015703933080658317\n",
      "Epoch [30/100], Loss: 0.01555878110229969\n",
      "Epoch [31/100], Loss: 0.01533842773642391\n",
      "Epoch [32/100], Loss: 0.015410715714097023\n",
      "Epoch [33/100], Loss: 0.0153037765994668\n",
      "Epoch [34/100], Loss: 0.015097945230081677\n",
      "Epoch [35/100], Loss: 0.014921584166586399\n",
      "Epoch [36/100], Loss: 0.014823172241449356\n",
      "Epoch [37/100], Loss: 0.014599324436858296\n",
      "Epoch [38/100], Loss: 0.014513049507513642\n",
      "Epoch [39/100], Loss: 0.014581064926460385\n",
      "Epoch [40/100], Loss: 0.014262685785070062\n",
      "Epoch [41/100], Loss: 0.014461722690612078\n",
      "Epoch [42/100], Loss: 0.014361762907356024\n",
      "Epoch [43/100], Loss: 0.014098027721047401\n",
      "Epoch [44/100], Loss: 0.0140578574500978\n",
      "Epoch [45/100], Loss: 0.013796868734061718\n",
      "Epoch [46/100], Loss: 0.013758712681010365\n",
      "Epoch [47/100], Loss: 0.013570332201197743\n",
      "Epoch [48/100], Loss: 0.013508101692423224\n",
      "Epoch [49/100], Loss: 0.013257564976811409\n",
      "Epoch [50/100], Loss: 0.013032968621701002\n",
      "Epoch [51/100], Loss: 0.013053119881078601\n",
      "Epoch [52/100], Loss: 0.012920353561639786\n",
      "Epoch [53/100], Loss: 0.012711019720882177\n",
      "Epoch [54/100], Loss: 0.012635566527023911\n",
      "Epoch [55/100], Loss: 0.012192040914669633\n",
      "Epoch [56/100], Loss: 0.012499049538746476\n",
      "Epoch [57/100], Loss: 0.012800285592675209\n",
      "Epoch [58/100], Loss: 0.013939903816208243\n",
      "Epoch [59/100], Loss: 0.015740923583507538\n",
      "Epoch [60/100], Loss: 0.017237158259376884\n",
      "Epoch [61/100], Loss: 0.013716254383325577\n",
      "Epoch [62/100], Loss: 0.013998071197420359\n",
      "Epoch [63/100], Loss: 0.013828841503709555\n",
      "Epoch [64/100], Loss: 0.01296735811047256\n",
      "Epoch [65/100], Loss: 0.012912963749840856\n",
      "Epoch [66/100], Loss: 0.012340360786765814\n",
      "Epoch [67/100], Loss: 0.011885743122547865\n",
      "Epoch [68/100], Loss: 0.01161903259344399\n",
      "Epoch [69/100], Loss: 0.011897899443283677\n",
      "Epoch [70/100], Loss: 0.010967934620566666\n",
      "Epoch [71/100], Loss: 0.011410166276618838\n",
      "Epoch [72/100], Loss: 0.011097571346908808\n",
      "Epoch [73/100], Loss: 0.010477711213752627\n",
      "Epoch [74/100], Loss: 0.010584293399006128\n",
      "Epoch [75/100], Loss: 0.010356066515669227\n",
      "Epoch [76/100], Loss: 0.01029713440220803\n",
      "Epoch [77/100], Loss: 0.010246849851682782\n",
      "Epoch [78/100], Loss: 0.010168119799345732\n",
      "Epoch [79/100], Loss: 0.010351197561249137\n",
      "Epoch [80/100], Loss: 0.011054439935833216\n",
      "Epoch [81/100], Loss: 0.009958830429241061\n",
      "Epoch [82/100], Loss: 0.00969876372255385\n",
      "Epoch [83/100], Loss: 0.009949971223250031\n",
      "Epoch [84/100], Loss: 0.00988114601932466\n",
      "Epoch [85/100], Loss: 0.010302426759153605\n",
      "Epoch [86/100], Loss: 0.009750006254762411\n",
      "Epoch [87/100], Loss: 0.009388543432578444\n",
      "Epoch [88/100], Loss: 0.009416276356205344\n",
      "Epoch [89/100], Loss: 0.009240487474016845\n",
      "Epoch [90/100], Loss: 0.009345704689621925\n",
      "Epoch [91/100], Loss: 0.009351138374768198\n",
      "Epoch [92/100], Loss: 0.008854072075337172\n",
      "Epoch [93/100], Loss: 0.009311899542808533\n",
      "Epoch [94/100], Loss: 0.009382960619404912\n",
      "Epoch [95/100], Loss: 0.008890339056961238\n",
      "Epoch [96/100], Loss: 0.010022004833444953\n",
      "Epoch [97/100], Loss: 0.009313899092376232\n",
      "Epoch [98/100], Loss: 0.00945920031517744\n",
      "Epoch [99/100], Loss: 0.009768936084583402\n",
      "Epoch [100/100], Loss: 0.00930381752550602\n",
      "모델 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from rnn_model import RNNModel\n",
    "\n",
    "# 수정된 PaddingCollate 클래스\n",
    "class PaddingCollate:\n",
    "    @staticmethod\n",
    "    def __call__(batch):\n",
    "        time = [item['data'][0] for item in batch]\n",
    "        pin1 = [item['data'][1] for item in batch]\n",
    "        pin2 = [item['data'][2] for item in batch]\n",
    "\n",
    "        max_length_pin1 = max(len(seq) for seq in pin1)\n",
    "        max_length_pin2 = max(len(seq) for seq in pin2)\n",
    "        max_length = max(max_length_pin1, max_length_pin2)\n",
    "\n",
    "        padded_pin1 = []\n",
    "        padded_pin2 = []\n",
    "\n",
    "        for seq1, seq2 in zip(pin1, pin2):\n",
    "            # 리스트를 텐서로 변환하여 패딩 수행\n",
    "            padded_pin1.append(PaddingCollate.pad_sequence(torch.tensor(seq1), max_length))\n",
    "            padded_pin2.append(PaddingCollate.pad_sequence(torch.tensor(seq2), max_length))\n",
    "\n",
    "        time_tensor = torch.tensor(time, dtype=torch.float)\n",
    "        padded_pin1_tensor = torch.stack(padded_pin1)\n",
    "        padded_pin2_tensor = torch.stack(padded_pin2)\n",
    "\n",
    "        return time_tensor, padded_pin1_tensor, padded_pin2_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padding_length = max_length - sequence.size(0)\n",
    "        padded_sequence = torch.nn.functional.pad(sequence, (0, padding_length))\n",
    "        return padded_sequence\n",
    "\n",
    "\n",
    "# 저장된 데이터셋 파일 이름\n",
    "file_name = 'concatenated_dataset'+'.pth'\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = torch.load(file_name)\n",
    "print(dataset)\n",
    "\n",
    "# DataLoader에 로드된 데이터셋 사용\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=PaddingCollate())\n",
    "\n",
    "# 학습 반복 횟수 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델 생성\n",
    "input_size = 2  # 입력 차원 설정 (pin1과 pin2)\n",
    "hidden_size = 64  # 은닉 상태의 크기 설정\n",
    "output_size = 1  # 출력 차원 설정\n",
    "num_layers = 2\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (time, pin1, pin2) in enumerate(dataloader):\n",
    "        # 모델 입력 준비\n",
    "        inputs = torch.stack([pin1, pin2], dim=2)  # pin1과 pin2를 합쳐서 입력으로 사용\n",
    "        inputs = inputs.float()  # 데이터 타입을 float으로 변환\n",
    "        targets = time.float()  # 타겟은 측정 시간으로 설정\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss 계산 및 Backpropagation\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Epoch마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')\n",
    "print(\"모델 저장이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb7f95d-22ce-484a-b19e-58abf4de64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_124736\\953195225.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pin1_seq = PaddingCollate.pad_sequence(torch.tensor(pin1_seq), max_length)\n",
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_124736\\953195225.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pin2_seq = PaddingCollate.pad_sequence(torch.tensor(pin2_seq), max_length)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RNNModel' object has no attribute 'num_directions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# 데이터 타입을 float으로 변환\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Loss 계산 및 Backpropagation\u001b[39;00m\n\u001b[0;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, time)  \u001b[38;5;66;03m# CrossEntropyLoss를 사용하므로 타겟은 라벨로 변경\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\graduationProject\\jupyter_lab\\rnn_model.py:15\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# 초기 은닉 상태 생성\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_directions, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# RNN 모델 실행\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h0)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RNNModel' object has no attribute 'num_directions'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from rnn_model2 import RNNModel\n",
    "from custom_dataset import CustomDataset\n",
    "\n",
    "# 수정된 PaddingCollate 클래스\n",
    "class PaddingCollate:\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padding_length = max_length - sequence.size(0)\n",
    "        padded_sequence = torch.nn.functional.pad(sequence, (0, padding_length))\n",
    "        return padded_sequence\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        times = []\n",
    "        pin1_seqs = []\n",
    "        pin2_seqs = []\n",
    "\n",
    "        # 최대 길이 계산\n",
    "        max_length_pin1 = max(len(sample['data'][1]) for sample in batch)\n",
    "        max_length_pin2 = max(len(sample['data'][2]) for sample in batch)\n",
    "        max_length = max(max_length_pin1, max_length_pin2)\n",
    "\n",
    "        for sample in batch:\n",
    "            # 시간 데이터 가져오기\n",
    "            time = sample['data'][0].unsqueeze(0)  # 차원을 추가하여 크기를 맞춤\n",
    "            pin1_seq = sample['data'][1]\n",
    "            pin2_seq = sample['data'][2]\n",
    "\n",
    "            # 각 시퀀스를 패딩하여 리스트에 추가\n",
    "            pin1_seq = PaddingCollate.pad_sequence(torch.tensor(pin1_seq), max_length)\n",
    "            pin2_seq = PaddingCollate.pad_sequence(torch.tensor(pin2_seq), max_length)\n",
    "\n",
    "            times.append(time)\n",
    "            pin1_seqs.append(pin1_seq.unsqueeze(0))  # 차원을 추가하여 크기를 맞춤\n",
    "            pin2_seqs.append(pin2_seq.unsqueeze(0))  # 차원을 추가하여 크기를 맞춤\n",
    "\n",
    "        # 리스트를 텐서로 변환\n",
    "        times_tensor = torch.cat(times, dim=0)\n",
    "        pin1_tensor = torch.cat(pin1_seqs, dim=0)\n",
    "        pin2_tensor = torch.cat(pin2_seqs, dim=0)\n",
    "\n",
    "        return times_tensor, pin1_tensor, pin2_tensor\n",
    "\n",
    "# 저장된 데이터셋 파일 이름\n",
    "file_name = 'concatenated_dataset.pth'\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = torch.load(file_name)\n",
    "\n",
    "# DataLoader에 로드된 데이터셋 사용\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=PaddingCollate())\n",
    "\n",
    "# 학습 반복 횟수 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델 생성\n",
    "input_size = 3  # 입력 차원 설정 (시간 + 핀1 + 핀2)\n",
    "hidden_size = 64  # 은닉 상태의 크기 설정\n",
    "output_size = 5  # 출력 차원 설정 (클래스 개수에 맞게 설정)\n",
    "num_layers = 2\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류에 적합한 CrossEntropyLoss 사용\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (time, pin1, pin2) in enumerate(dataloader):\n",
    "        # 모델 입력 준비\n",
    "        inputs = torch.cat([time.unsqueeze(1), pin1, pin2], dim=1)  # 시간, 핀1, 핀2를 합쳐서 입력으로 사용\n",
    "        inputs = inputs.float()  # 데이터 타입을 float으로 변환\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss 계산 및 Backpropagation\n",
    "        loss = criterion(outputs, time)  # CrossEntropyLoss를 사용하므로 타겟은 라벨로 변경\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Epoch마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')\n",
    "print(\"모델 저장이 완료되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
